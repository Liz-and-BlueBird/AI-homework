# 实践要求：

![](image\0.png)

# 准备工作：

使用pygame模块构建五子棋程序，棋盘大小15x15，玩家先手执黑子，电脑执白子且随机落子

![](image\1.png)

获胜时会打印信息，下图是黑子获胜的情形，白子类似

![](image\2.png)

# 使用监督学习算法识别五子棋落子位置：

## 数据集：

随机生成棋盘图像作为数据集，其中，训练集128张，验证集和测试集各32张，标签保存为npy文件，下图是训练集中的一个样本

![](image\3.png)

由于是随机生成的，所以落子位置看起来杂乱无章

## 神经网络：

由于是简单的图像识别，所以采用卷积神经网络，网络结构包括卷积层、池化层和线性层

## 核心设计：

网络输入为600x600的图像（初始图像大小为680x680，经过中心裁剪后变为600x600），每一个棋子占据40x40的方块，此时可以将整个棋盘划分为225个相同大小的棋块，由此设计卷积层卷积核大小为20x20，步长20，这样每个棋块由原来的40x40变为2x2，在接下来的池化层中对这样的2x2棋块进行平均池化，线性层输出每个棋块的识别结果，无子，黑子或白子

## 具体参数：

卷积层：输入通道数3（RGB），输出通道6（向高维映射），卷积核大小20，步长20

平均池化层：核大小2，步长2

线性连接层：输入特征数6（对应卷积层的输出通道数），输出特征数3（无子，黑子或白子）

## 训练过程：

利用梯度下降优化网络参数，迭代10轮，批量大小8，学习率0.05

训练过程打印信息如下

![](image\4.png)

## 结果分析：

在训练集和验证集上模型均达到了百分之百的正确率，原因有两点：一是输入图像的参数非常”固定“，对于划分的棋块之间，无子，黑子或白子，只要状态是相同的，转换成网络输入数据时，RGB取值也都是相同的，也就是说，输入本身没有”例外“，模型不需要泛化性来应对没见过的输入；二是针对这样的输入设计的划分为相同棋块的方法，这有点类似于滑动方块，棋块的种类其实只有三种，无子，黑子和白子。基于这两点，神经网络的正确率不难达到百分之百，而且从训练过程打印的信息来看，模型很快就完成了收敛

## 改进措施：

平均池化层可以改为最大池化层，虽然就结果来看，两者相同，但棋块的设计中，如果落有棋子，棋块内大部分区域由棋子占据，最大池化逻辑上比平均池化更能捕捉到这一点

# 博弈搜索，人为设定函数：

## 主要过程：

五子棋是比较简单的博弈过程，利用极大极小搜索和判断落子位置

## 核心设计：

该方法的核心在于评价函数的设计，对于待落子方，在当前棋局状态下进行落子，对每一个可落子的位置给出它的”优劣“性，选取其中最”优“的一个位置落子。实验设定的评价函数主要依据是棋盘上每一个棋子周围棋子的分布情况，假设当前步已落子后，对于当前棋盘上任意一个棋子，它在水平、竖直和两条斜对角线，共四个方向可以产生5子连线，分别扫描这四个方向，对每个方向上棋子分布情况进行分析。以水平为例，取包括该棋子在内左右对称的共9个位置，连续5个位置为一组，那么就有5组，这里需要注意的是每一组都含有这一个棋子。对于每一组，如果存在对手棋子，则己方无可能5子连线，但同时对方也无可能5子连线，统计对方棋子个数，越多则说明抑制程度越大；如果不存在对手棋子，则己方有可能5子连线，属于优势局面，己方棋子个数平方累加，平方的目的是实现构成5子连线优先级大于封锁对手行动。最后的评价值为后者减去前者，越大则说明在这套分析理论下，该棋局状态对己方越有利，对应的落子位置也就越“优”

## 结果展示：

测试的几次对弈中，算法表现出一定的”智能“，对棋局的判断比较中肯，不论是阻止玩家达到4子连线的优势局面，还是己方建立优势，都能有所取舍

下面一组图为一次实际对弈过程

![](image\5.png)![](image\6.png)![](image\7.png)![](image\8.png)![](image\9.png)![](image\10.png)![](image\11.png)![](image\12.png)![](image\13.png)![](image\14.png)

## 分析改进：

搜索深度会影响电脑”智能“程度和落子速度

当搜索深度为1时，每次落子平均耗时1.3sec，算法比较“幼稚”，大部分情况下只顾自己5子连线，对玩家落子位置注意较少

当搜索深度为2时，每次落子平均耗时2.5sec，算法已经可以正常和玩家对弈，和深度1比起来有明显进步

当搜索深度为3时，每次落子平均耗时2.9sec，算法智能程度与深度2相比提升并不明显

实验中设定评价函数使用的分析方法基于个人理解，在同学介绍后发现五子棋对特定棋子分布有专门的称呼，如“跳三”、“活四”等，这些特定棋子分布对5子连线有着重要影响，依此设计评价函数可能更为科学

有时候，算法对于玩家必胜局面，即已存在4子连线且两端均可落子的情况下，获取落子位置时间明显变长，可能是由于没有合适的落子位置，算法计算量突增，而且最后落子位置依旧不尽如人意，实验中后续对这种情况单独讨论，4子连线并没有进入博弈搜索

# 神经网络判断棋局状态，进化计算进行学习：

## 网络设计：

依旧采用卷积神经网络，网络结构包括归一层、卷积层、最大池化层和线性连接层，输入是具有结构信息的棋盘数组，所以使用卷积神经网络捕捉特征

## 细节处理：

输入是15x15的棋盘数组，由最初的识别落子位置的神经网络输出转换而来，为了使网络能够应对先后手，己方棋子特征为1，对方棋子特征为-1，未落子特征为0，在送入神经网络前加入正态分布的小量，目的是使网络归一化结果显示差异

卷积核大小为3，步长为1，目的是尽可能捕捉3子及以上棋子连线的特征

池化核大小为2，步长为1，这里更多地是猜测，和上述卷积核组合捕捉4x4方块内的主要棋子特征

输出维度大小为225，对应每一个位置落子的相对可能性（非概率），排除已落子位置，在剩余位置中选取值最大者作为网络判断结果

## 进化过程：

使用进化规划算法，算法的基本流程不变

初始群体：5个个体，即5组网络，各自有着随机初始化的参数

环境：个体需要对弈的对手，也是1组网络，用来计算个体适应度，即下棋水平，方法为统计对弈胜率，初始环境随机生成，进化过程中每隔一定代数会更新为最优个体

突变：对网络参数进行调整，算子为标准进化规划中的突变算子，但适应度改为对弈败率，这样做主要是考虑水平越高的个体变化程度就越小

选择：q-竞争优化，实验中q值取为5，选取对弈胜率前5个个体作为新一代群体

## 结果分析：

下图为进化20代的过程信息

![](image\15.png)

单从数据来看，不论是个体水平，还是群体水平，都有所提升，但实际对弈时电脑落子依旧杂乱无章，原因应该在于进化轮数不够，20代过少，这段时期，不论是个体，还是环境，对弈几乎都是随处乱下，然后可能某一次恰好5子连线，获得胜利，这种情况是很难谈电脑是“智能”的，是按一定的策略去落子的

在此基础上，根本的改进措施为加大进化轮数，但由于时间和设备所限，实验止步于20代

## 参数调整：

不能进行更多次进化是因为每一轮进化耗时过长，实验中也为此调整过一些进化参数，比如适应度的计算中需要设定对弈轮数，由最初的20次改为最后的5次，时间虽然大大缩短，但随机性也增加了，特别是在进化初期，很难说适应度就反映了个体下棋水平；另外就是环境更新的周期，测试10代更新一次与5代更新一次对结果数据没有明显的影响，这或许也是由于进化轮数过少而显示不出差异

# 强化学习进行学习：

## 网络设计：

由于使用深度强化学习，在网络设计上做了调整，主体依旧采用卷积神经网络，包括卷积层、归一层和线性层

卷积层：输出维度为3，卷积核大小为3，理由也是捕捉3子及以上棋子连线的特征

归一层：在批量大小维度上进行归一化，主要是加快收敛速度

线性层：产生输出，获取最后落子位置

## 学习算法：

实验采用的Nature DQN算法主要流程如下

![](image\16.png)

Nature DQN在DQN的基础上增加一个Q网络，当前Q网络Q用来选择动作，更新模型参数，目标Q网络Q′用于计算目标Q值。目标Q网络的网络参数不需要迭代更新，而是每隔一段时间从当前Q网络复制过来，目的是减少目标Q值和当前Q值的相关性。

实验中通过电脑之间不断进行对弈，对弈过程产生的棋盘状态，作为经验储入经验池，用于后续强化学习

## 奖励设置：

奖励分为两部分，一是是否扩大了己方最大共线棋子个数，比重0.6，二是是否抑制了对方共线可能性，比重0.4，这样设计的目的是兼顾促使己方5子连线与在必要时阻止对方5子连线，即这两种行为都可以得到一定程度的奖励

## 结果分析：

下图为对局50次的过程信息，每次对局后网络都会进行更新

![](image\17.png)![](image\18.png)![](image\19.png)

损失值的变化并没有明显的下降趋势，推测原因在于学习轮数不够，电脑之间对弈初期随机性大于策略性，实际对弈中电脑表现出的也是明显的随机性

## 改进措施：

主要改进措施还是增加学习轮数，看在更高数量级的学习轮数下模型是否展现一定的策略性

另外，奖励设置可以改为只有获胜时才会得到奖励，这样可能有助于加快对获胜这一目标的收敛

# 目录结构说明：

Class.py：定义程序需要使用的各种类，也是程序运行入口

CNN.py：对应监督学习的训练过程

SelectCNN.py：对应进化计算进行学习的过程

DQN.py：对应强化学习的过程

test，val，test：监督学习数据集

tmp：程序运行中保存临时的图像数据

image：一些图片